# First node code 
import pickle
import os
import re
from nltk.corpus import stopwords
from flask import Flask, request, jsonify

# --- Globals and Initialization ---
MODEL_FILE = 'models/spam_classifier_model.pkl'
VECTORIZER_FILE = 'models/tfidf_vectorizer.pkl'
FLASK_PORT = 5000 

# Load stopwords list
try:
    STOP_WORDS = set(stopwords.words('english'))
except LookupError:
    import nltk
    nltk.download('stopwords', quiet=True)
    STOP_WORDS = set(stopwords.words('english'))


# Load the saved model and vectorizer
try:
    with open(VECTORIZER_FILE, 'rb') as file:
        tfidf = pickle.load(file)
    with open(MODEL_FILE, 'rb') as file:
        model = pickle.load(file)
    print("Models loaded successfully.")
except FileNotFoundError:
    print(f"ERROR: Model files not found. Run train_model.py first.")
    exit()

app = Flask(__name__)

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text) 
    text = ' '.join(word for word in text.split() if word not in STOP_WORDS)
    return text

# --- API Endpoint ---
@app.route('/predict', methods=['GET'])
def predict():
    """API Endpoint: /predict?text=..."""
    
    # Enable CORS for local testing (CRITICAL for index.html to work)
    if request.method == 'OPTIONS':
        response = app.make_default_options_response()
    else:
        input_text = request.args.get('text')
        if not input_text:
            response = jsonify({'status': 'error', 'message': 'Missing text parameter'}), 400
        else:
            try:
                processed_text = clean_text(input_text)
                vectorized_text = tfidf.transform([processed_text])
                prediction_index = model.predict(vectorized_text)[0]
                prediction_label = 'spam' if prediction_index == 1 else 'not-spam'
                probabilities = model.predict_proba(vectorized_text)[0]
                confidence = probabilities[prediction_index]

                response = jsonify({
                    'status': 'success',
                    'input_text': input_text,
                    'prediction': prediction_label,
                    'confidence_score': round(confidence, 4) 
                })
            except Exception as e:
                response = jsonify({'status': 'error', 'message': f'An error occurred: {str(e)}'}), 500

    # Add the CORS header for ALL responses
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Methods'] = 'GET, OPTIONS'
    response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
    return response


if __name__ == '__main__':
    print(f"Flask API starting on http://127.0.0.1:{FLASK_PORT}")
    # The debug=True setting is helpful during local development
    app.run(host='xxx.x.x.x', port=FLASK_PORT, debug=True)





# Second node code
import requests
import json

# Your Flask app is running internally on this address
TEST_URL = 'http://xxx.x.x.x:xx00/predict'

# Test Message
message = "Congratulations! You won $1000 cash prize!"

print("--- Testing API Endpoint Internally ---")

try:
    # Send request to the running Flask server
    response = requests.get(TEST_URL, params={'text': message}, timeout=5)
    response.raise_for_status() # Raise error for bad status codes

    data = response.json()

    print("\n=======================================================")
    print("‚úÖ API Endpoint Test Successful!")
    print(f"Input: {data['input_text'][:30]}...")
    print(f"Prediction: {data['prediction'].upper()}")
    print(f"Confidence: {data['confidence_score'] * 100:.2f}%")
    print("=======================================================")
    print("This proves the backend code works perfectly.")

except requests.exceptions.RequestException as e:
    print(f"‚ùå Backend Test Failed: Could not connect to internal server. Details: {e}")



# Third node code
# ==============================================================================
# 1. SETUP & LIBRARIES
# ==============================================================================
print("1. Installing necessary libraries...")
# Install required libraries + Gradio
!pip install pandas scikit-learn nltk kagglehub gradio -qq

from IPython.display import display, HTML # Required for rendering the UI
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
import pickle
import re
import os
import kagglehub
import nltk
from nltk.corpus import stopwords
import gradio as gr # Gradio is the tool we are using for the interactive UI

# Define file paths
MODEL_FILE = 'spam_classifier_model.pkl'
VECTORIZER_FILE = 'tfidf_vectorizer.pkl'
STOP_WORDS = set() 

# --- Model Setup and Training ---
def setup_and_train():
    """Handles model training and saving artifacts."""
    global STOP_WORDS
    try:
        stopwords.words('english')
    except LookupError:
        nltk.download('stopwords', quiet=True)
    
    STOP_WORDS = set(stopwords.words('english'))
    
    # Data Loading and Cleaning
    print("2. Downloading data and training model...")
    download_path = kagglehub.dataset_download("uciml/sms-spam-collection-dataset")
    data_path = os.path.join(download_path, 'spam.csv')
    df = pd.read_csv(data_path, encoding='latin-1') 
    df = df.rename(columns={'v1': 'label', 'v2': 'text'})
    df = df.drop(columns=df.columns[2:]) 
    df['label'] = df['label'].map({'ham': 0, 'spam': 1})

    def clean_text(text):
        text = text.lower()
        text = re.sub(r'[^a-z\s]', '', text) 
        text = ' '.join(word for word in text.split() if word not in STOP_WORDS)
        return text

    df['cleaned_text'] = df['text'].apply(clean_text)

    # Training
    X = df['cleaned_text']
    y = df['label']
    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X_vec = tfidf.fit_transform(X)
    model = MultinomialNB(alpha=1.0) 
    model.fit(X_vec, y) 
    
    # Saving artifacts
    with open(VECTORIZER_FILE, 'wb') as f: pickle.dump(tfidf, f)
    with open(MODEL_FILE, 'wb') as f: pickle.dump(model, f)
    
    print("Model Training Complete.")
    
setup_and_train()


# --- 3. THE CORE PREDICTION FUNCTION (Using Saved Artifacts) ---

# Load the saved model and vectorizer
try:
    with open(VECTORIZER_FILE, 'rb') as file: tfidf = pickle.load(file)
    with open(MODEL_FILE, 'rb') as file: model = pickle.load(file)
except FileNotFoundError:
    print("Error loading models.")
    exit()

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text) 
    text = ' '.join(word for word in text.split() if word not in STOP_WORDS)
    return text

def predict_spam(input_text):
    """The function that runs the classification logic."""
    if not input_text:
        return "Please enter a message."
    
    # 1. Clean and Vectorize
    processed_text = clean_text(input_text)
    vectorized_text = tfidf.transform([processed_text])
    
    # 2. Predict Class and Confidence
    prediction_index = model.predict(vectorized_text)[0]
    prediction_label = 'SPAM' if prediction_index == 1 else 'NOT-SPAM (HAM)'
    
    probabilities = model.predict_proba(vectorized_text)[0]
    confidence = probabilities[prediction_index]

    # 3. Format the result for the UI
    result_text = (
        f"CLASSIFICATION: {prediction_label}\n"
        f"Confidence: {confidence * 100:.2f}%"
    )
    
    # Use color based on prediction
    color = "red" if prediction_index == 1 else "green"
    
    return f'<p style="color: {color}; font-size: 18px; font-weight: bold;">{result_text}</p>'


# ==============================================================================
# 4. LAUNCH INTERACTIVE WEB PAGE (Gradio)
# ==============================================================================
print("\n4. Launching guaranteed interactive web page...")

# Gradio creates the frontend structure and connects it directly to the Python function.
iface = gr.Interface(
    fn=predict_spam,
    inputs=gr.Textbox(lines=4, placeholder="Enter SMS message here..."),
    outputs=gr.HTML(), # Use HTML output to display colored results
    title="üì± SMS Spam Classifier",
    description="This interface uses a trained Multinomial Naive Bayes model to classify messages.",
    # --- FIX APPLIED HERE ---
    flagging_mode="never" 
    # -------------------------
)

# Launch the interface, which creates an embedded interactive UI in the Colab output.
# The `share=True` option will provide an external link if needed, but it works embedded.
iface.launch(inbrowser=True, share=False)
